<div align="center">

  <h1 align="center">
        Awesome-LLM-Related-Papers-Comprehensive-Topics
    </h1>

  <a href="https://shure-dev.github.io/"><img alt="Static Badge" src="https://img.shields.io/badge/ProjectPage-blue"></a>
  <a href="https://potent-twister-29f.notion.site/b0fc32542854456cbde923e0adb48845?v=e2d14d2ef0c848f5a1d5b71f9977d7c5&pvs=4"><img alt="Static Badge" src="https://img.shields.io/badge/TableOnNotion-gray"></a>
  </a>
  <a href="https://github.com/shure-dev/Awesome-LLM-related-Papers-Comprehensive-Topics">
  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/shure-dev/Awesome-LLM-related-Papers-Comprehensive-Topics"></a>

  <p align="center">
        We provide awesome papers and repos on very comprehensive topics as follows.
  </p>
  
  <p align="center">
    <code>CoT / VLM / Quantization / Grounding / Text2IMG&VID / Prompt Engineering / Prompt Tuning / Reasoning / Robot / Agent / Planning / Reinforcement-Learning / Feedback / In-Context-Learning / Few-Shot / Zero-Shot / Instruction Tuning / PEFT / RLHF / RAG / Embodied / VQA / Hallucination / Diffusion / Scaling / Context-Window / WorldModel / Memory / Zero-Shot / RoPE / Speech / Perception / Survey / Segmentation / Learge Action Model / Foundation / RoPE / LoRA / PPO / DPO </code>
  </p>
  

<p align="center">
    We strongly recommend checking our Notion table for an interactive experience.
</p>

  <a href="https://github.com/shure-dev/Awesome-LLM-Papers-Toward-AGI/assets/61527175/84447574-1134-4b0d-8442-f509162da10e">

  <img src="https://github.com/shure-dev/Awesome-LLM-Papers-Toward-AGI/assets/61527175/84447574-1134-4b0d-8442-f509162da10e" alt="drawing" width="700"/>
  </a>
  
</div>

<br>

<div align="center"> Number of papers and repos in total: 516</div>
<br>

| Category | Title | Links | Date |
| --- | --- | --- | --- |
| Zero-shot | Can Foundation Models Perform Zero-Shot Task Speci<br>fication For Robot Manipulation? |  |  |
| World-model | Leveraging Pre-trained Large Language Models to Co<br>nstruct and Utilize World Models for Model-based Task Planning | [ArXiv](https://arxiv.org/pdf/2305.14909) | 2023/05/07 |
| World-model | Learning and Leveraging World Models in Visual Rep<br>resentation Learning |  |  |
| World-model | Language Models Meet World Models | [ArXiv](https://arxiv.org/abs/2305.10626) |  |
| World-model | Learning to Model the World with Language | [ArXiv](https://arxiv.org/abs/2308.01399) |  |
| World-model | Diffusion World Model | [ArXiv](https://arxiv.org/abs/2402.03570) |  |
| World-model | Learning to Model the World with Language | [ArXiv](https://arxiv.org/abs/2308.01399) |  |
| VisualPrompt | Ferret-v2: An Improved Baseline for Referring and <br>Grounding with Large Language Models | [ArXiv](https://arxiv.org/abs/2404.07973) |  |
| VisualPrompt | Making Large Multimodal Models Understand Arbitrar<br>y Visual Prompts | [ArXiv](https://arxiv.org/abs/2312.00784) |  |
| VisualPrompt | What does CLIP know about a red circle? Visual pro<br>mpt engineering for VLMs | [ArXiv](https://arxiv.org/abs/2304.06712) |  |
| VisualPrompt | MOKA: Open-Vocabulary Robotic Manipulation through<br> Mark-Based Visual Prompting | [ArXiv](https://arxiv.org/abs/2403.03174) |  |
| VisualPrompt | SoM : Set-of-Mark PromptingUnleashes Extraordinary<br> Visual Grounding in GPT-4V | [ArXiv](https://arxiv.org/pdf/2310.11441) |  |
| VisualPrompt | Set-of-Mark Prompting Unleashes Extraordinary Visu<br>al Grounding in GPT-4V | [ArXiv](https://arxiv.org/abs/2310.11441), [GitHub](https://som-gpt4v.github.io/) |  |
| Video | MA-LMM: Memory-Augmented Large Multimodal Model fo<br>r Long-Term Video Understanding | [ArXiv](https://arxiv.org/abs/2404.05726) |  |
| ViFM, Video | InternVideo2: Scaling Video Foundation Models for <br>Multimodal Video Understanding | [ArXiv](https://arxiv.org/abs/2403.15377), [GitHub](https://github.com/OpenGVLab/InternVideo2/) |  |
| VLM, World-model | Large World Model | [ArXiv](https://arxiv.org/abs/2402.08268) |  |
| VLM, VQA | CogVLM: Visual Expert for Pretrained Language Mode<br>ls | [ArXiv](https://arxiv.org/abs/2311.03079) | 2023/11/06 |
| VLM, VQA | Chameleon: Plug-and-Play Compositional Reasoning w<br>ith Large Language Models | [ArXiv](https://arxiv.org/abs/2304.09842) | 2023/04/19 |
| VLM, VQA | DeepSeek-VL: Towards Real-World Vision-Language Un<br>derstanding01 |  |  |
| VLM | PaLM: Scaling Language Modeling with Pathways | [ArXiv](https://arxiv.org/abs/2204.02311) | 2022/04/05 |
| VLM | ScreenAI: A Vision-Language Model for UI and Infog<br>raphics Understanding |  |  |
| VLM | MoE-LLaVA: Mixture of Experts for Large Vision-Lan<br>guage Models | [ArXiv](https://arxiv.org/abs/2401.15947), [GitHub](https://github.com/PKU-YuanGroup/MoE-LLaVA) |  |
| VLM | LLaVA-NeXT: Improved reasoning, OCR, and world kno<br>wledge | [GitHub](https://llava-vl.github.io/blog/2024-01-30-llava-next/) |  |
| VLM | Mini-Gemini: Mining the Potential of Multi-modalit<br>y Vision Language Models | [ArXiv](https://arxiv.org/pdf/2403.18814.pdf) |  |
| Text-to-Image, World-model | World Model on Million-Length Video And Language W<br>ith RingAttention | [ArXiv](https://arxiv.org/abs/2402.08268) |  |
| Tex2Img | Be Yourself: Bounded Attention for Multi-Subject T<br>ext-to-Image Generation | [ArXiv](https://arxiv.org/abs/2403.16990) |  |
| Temporal | Explorative Inbetweening of Time and Space | [ArXiv](https://arxiv.org/abs/2403.14611) |  |
| Survey, Video | Video Understanding with Large Language Models: A <br>Survey | [ArXiv](https://arxiv.org/pdf/2312.17432) |  |
| Survey, VLM | MM-LLMs: Recent Advances in MultiModal Large Langu<br>age Models |  |  |
| Survey, Training | Understanding LLMs: A Comprehensive Overview from <br>Training to Inference | [ArXiv](https://arxiv.org/pdf/2401.02038) |  |
| Survey, TimeSeries | Large Models for Time Series and Spatio-Temporal D<br>ata: A Survey and Outlook |  |  |
| Survey | Efficient Large Language Models: A Survey | [ArXiv](https://arxiv.org/abs/2312.03863), [GitHub](https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey) |  |
| Sora, Text-to-Video | Sora: A Review on Background, Technology, Limitati<br>ons, and Opportunities of Large Vision Models |  |  |
| Sora, Text-to-Video | Mora: Enabling Generalist Video Generation via A M<br>ulti-Agent Framework | [ArXiv](https://arxiv.org/abs/2403.13248) |  |
| Segmentation | LISA: Reasoning Segmentation via Large Language Mo<br>del | [ArXiv](https://arxiv.org/pdf/2308.00692.pdf) |  |
| Segmentation | GRES: Generalized Referring Expression Segmentatio<br>n |  |  |
| Segmentation | Generalized Decoding for Pixel, Image, and Languag<br>e | [ArXiv](https://openaccess.thecvf.com/content/CVPR2023/papers/Zou_Generalized_Decoding_for_Pixel_Image_and_Language_CVPR_2023_paper.pdf) |  |
| Segmentation | SEEM: Segment Everything Everywhere All at Once | [ArXiv](https://arxiv.org/abs/2304.06718), [GitHub](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once) |  |
| Segmentation | SegGPT: Segmenting Everything In Context | [ArXiv](https://arxiv.org/pdf/2304.03284) |  |
| Segmentation | Grounded SAM: Assembling Open-World Models for Div<br>erse Visual Tasks | [ArXiv](https://arxiv.org/abs/2401.14159) |  |
| Scaling | Leave No Context Behind: Efficient Infinite Contex<br>t Transformers with Infini-attention | [ArXiv](https://arxiv.org/abs/2404.07143) |  |
| SLM, Scaling | Textbooks Are All You Need | [ArXiv](https://arxiv.org/abs/2306.11644) |  |
| Robot, Zero-shot | BC-Z: Zero-Shot Task Generalization with Robotic I<br>mitation Learning | [ArXiv](https://arxiv.org/abs/2202.02005) |  |
| Robot, Zero-shot | Universal Manipulation Interface: In-The-Wild Robo<br>t Teaching Without In-The-Wild Robots | [ArXiv](https://arxiv.org/pdf/2402.10329) |  |
| Robot, Zero-shot | Mirage: Cross-Embodiment Zero-Shot Policy Transfer<br> with Cross-Painting | [ArXiv](https://arxiv.org/pdf/2402.19249) |  |
| Robot, Task-Decompose, Zero-shot | Language Models as Zero-Shot Planners: Extracting <br>Actionable Knowledge for Embodied Agents | [ArXiv](https://arxiv.org/abs/2201.07207) | 2022/01/18 |
| Robot, Task-Decompose | SayPlan: Grounding Large Language Models using 3D <br>Scene Graphs for Scalable Robot Task Planning | [ArXiv](https://arxiv.org/abs/2307.06135) | 2023/07/12 |
| Robot, TAMP | LLM3:Large Language Model-based Task and Motion Pl<br>anning with Motion Failure Reasoning | [ArXiv](https://arxiv.org/pdf/2403.11552) |  |
| Robot, TAMP | Task and Motion Planning with Large Language Model<br>s for Object Rearrangement | [ArXiv](https://arxiv.org/abs/2303.06247) |  |
| Robot, Survey | Toward General-Purpose Robots via Foundation Model<br>s: A Survey and Meta-Analysis | [ArXiv](https://arxiv.org/abs/2312.08782) | 2023/12/14 |
| Robot, Survey | Language-conditioned Learning for Robotic Manipula<br>tion: A Survey | [ArXiv](https://arxiv.org/abs/2312.10807) | 2023/12/17 |
| Robot, Survey | Robot Learning in the Era of Foundation Models: A <br>Survey | [ArXiv](https://arxiv.org/abs/2311.14379) | 2023/11/24 |
| Robot, Survey | Real-World Robot Applications of Foundation Models<br>: A Review | [ArXiv](https://arxiv.org/pdf/2402.05741) |  |
| Robot | OK-Robot: What Really Matters in Integrating Open-<br>Knowledge Models for Robotics | [ArXiv](https://arxiv.org/pdf/2401.12202) |  |
| Robot | RoCo: Dialectic Multi-Robot Collaboration with Lar<br>ge Language Models | [ArXiv](https://arxiv.org/abs/2307.04738) |  |
| Robot | Interactive Language: Talking to Robots in Real Ti<br>me | [ArXiv](https://arxiv.org/pdf/2210.06407) |  |
| Robot | Reflexion: Language Agents with Verbal Reinforceme<br>nt Learning | [ArXiv](https://arxiv.org/abs/2303.11366) | 2023/03/20 |
| Robot | Generative Expressive Robot Behaviors using Large <br>Language Models | [ArXiv](https://arxiv.org/pdf/2401.14673) |  |
| Robot | RoboCat: A Self-Improving Generalist Agent for Rob<br>otic Manipulation |  |  |
| Robot | Introspective Tips: Large Language Model for In-Co<br>ntext Decision Making | [ArXiv](https://www.semanticscholar.org/paper/Introspective-Tips%3A-Large-Language-Model-for-Making-Chen-Wang/047e3812854a86b2a2e113219fa956eda860ce24) |  |
| Robot | PIVOT: Iterative Visual Prompting Elicits Actionab<br>le Knowledge for VLMs | [ArXiv](https://arxiv.org/abs/2402.07872) |  |
| Robot | OCI-Robotics: Object-Centric Instruction Augmentat<br>ion for Robotic Manipulation | [ArXiv](https://arxiv.org/abs/2401.02814) |  |
| Robot | DeliGrasp: Inferring Object Mass, Friction, and Co<br>mpliance with LLMs for Adaptive and Minimally Deforming Grasp Policies | [ArXiv](https://arxiv.org/abs/2403.07832) |  |
| Robot | VoxPoser: Composable 3D Value Maps for Robotic Man<br>ipulation with Language Models | [ArXiv](https://arxiv.org/abs/2307.05973) |  |
| Robot | Creative Robot Tool Use with Large Language Models | [ArXiv](https://arxiv.org/abs/2310.13065) |  |
| Robot | AutoTAMP: Autoregressive Task and Motion Planning <br>with LLMs as Translators and Checkers | [ArXiv](https://arxiv.org/pdf/2306.06531) |  |
| RoPE | RoFormer: Enhanced Transformer with Rotary Positio<br>n Embedding | [ArXiv](https://arxiv.org/abs/2104.09864) |  |
| Resource | [Resource] Paperswithcode | [ArXiv](https://paperswithcode.com/) |  |
| Resource | [Resource] huggingface | [ArXiv](https://huggingface.co/papers) |  |
| Resource | [Resource] dailyarxiv | [ArXiv](https://dailyarxiv.com/) |  |
| Resource | [Resource] Connectedpapers | [ArXiv](https://www.connectedpapers.com/) |  |
| Resource | [Resource] Semanticscholar | [ArXiv](https://www.semanticscholar.org) |  |
| Resource | [Resource] AlphaSignal | [ArXiv](https://alphasignal.ai/) |  |
| Resource | [Resource] arxiv-sanity | [ArXiv](https://arxiv-sanity-lite.com/) |  |
| Reinforcement-Learning, VIMA | FoMo Rewards: Can we cast foundation models as rew<br>ard functions? | [ArXiv](https://arxiv.org/pdf/2312.03881) |  |
| Reinforcement-Learning, Robot | Towards A Unified Agent with Foundation Models | [ArXiv](https://arxiv.org/abs/2307.09668) |  |
| Reinforcement-Learning | Large Language Models Are Semi-Parametric Reinforc<br>ement Learning Agents | [ArXiv](https://arxiv.org/pdf/2306.07929) |  |
| Reinforcement-Learning | RLang: A Declarative Language for Describing Parti<br>al World Knowledge to Reinforcement Learning Agents | [ArXiv](https://arxiv.org/pdf/2208.06448) |  |
| Reasoning, Zero-shot | Large Language Models are Zero-Shot Reasoners | [ArXiv](https://arxiv.org/pdf/2205.11916) |  |
| Reasoning, VLM, VQA | MM-REACT: Prompting ChatGPT for Multimodal Reasoni<br>ng and Action | [ArXiv](https://arxiv.org/abs/2303.11381) | 2023/03/20 |
| Reasoning, Table | Large Language Models are few(1)-shot Table Reason<br>ers | [ArXiv](https://arxiv.org/abs/2210.06710) |  |
| Reasoning, Symbolic | Symbol-LLM: Leverage Language Models for Symbolic <br>System in Visual Human Activity Reasoning | [ArXiv](https://arxiv.org/pdf/2311.17365) |  |
| Reasoning, Survey | Reasoning with Language Model Prompting: A Survey | [ArXiv](https://arxiv.org/abs/2212.09597) |  |
| Reasoning, Robot | AlphaBlock: Embodied Finetuning for Vision-Languag<br>e Reasoning in Robot Manipulation | [ArXiv](https://arxiv.org/pdf/2305.18898) |  |
| Reasoning, Reward | LET’S REWARD STEP BY STEP: STEP-LEVEL REWARD MODEL<br> AS THE NAVIGATORS FOR REASONING | [ArXiv](https://openreview.net/pdf?id=RSQL6xvUYW) |  |
| Reasoning, Reinforcement-Learning | ReFT: Reasoning with Reinforced Fine-Tuning |  |  |
| Reasoning | Selection-Inference: Exploiting Large Language Mod<br>els for Interpretable Logical Reasoning | [ArXiv](https://arxiv.org/abs/2205.09712) |  |
| Reasoning | ReConcile: Round-Table Conference Improves Reasoni<br>ng via Consensus among Diverse LLMs. | [ArXiv](https://arxiv.org/pdf/2309.13007) |  |
| Reasoning | Self-Discover: Large Language Models Self-Compose <br>Reasoning Structures | [ArXiv](https://arxiv.org/pdf/2402.03620) |  |
| Reasoning | Chain-of-Thought Reasoning Without Prompting | [ArXiv](https://arxiv.org/abs/2402.10200) |  |
| Reasoning | Contrastive Chain-of-Thought Prompting | [ArXiv](https://arxiv.org/pdf/2311.09277) |  |
| Reasoning | Rephrase and Respond(RaR) |  |  |
| Reasoning | Take a Step Back: Evoking Reasoning via Abstractio<br>n in Large Language Models | [ArXiv](https://arxiv.org/pdf/2310.06117) |  |
| Reasoning | STaR: Bootstrapping Reasoning With Reasoning | [ArXiv](https://arxiv.org/abs/2203.14465) | 2022/05/28 |
| Reasoning | The Impact of Reasoning Step Length on Large Langu<br>age Models | [ArXiv](https://arxiv.org/pdf/2401.04925) |  |
| Reasoning | Beyond Natural Language: LLMs Leveraging Alternati<br>ve Formats for Enhanced Reasoning and Communication | [ArXiv](https://arxiv.org/abs/2402.18439) |  |
| Reasoning | Large Language Models as General Pattern Machines | [ArXiv](https://arxiv.org/abs/2307.04721) |  |
| RLHF, Reinforcement-Learning, Survey | A Survey of Reinforcement Learning from Human Feed<br>back |  |  |
| RLHF | Secrets of RLHF in Large Language Models Part II: <br>Reward Modeling | [ArXiv](https://arxiv.org/pdf/2401.06080) |  |
| RAG, Temporal Logics | FreshLLMs: Refreshing Large Language Models with S<br>earch Engine Augmentation | [ArXiv](https://arxiv.org/abs/2310.03214) |  |
| RAG, Survey | Large Language Models for Information Retrieval: A<br> Survey |  |  |
| RAG, Survey | Retrieval-Augmented Generation for Large Language  |  |  |
| RAG, Survey | Retrieval-Augmented Generation for Large Language <br>Models: A Survey | [ArXiv](https://arxiv.org/pdf/2312.10997) |  |
| RAG | Training Language Models with Memory Augmentation |  |  |
| RAG | Self-RAG: Learning to Retrieve, Generate, and Crit<br>ique through Self-Reflection |  |  |
| RAG | RAG-Fusion: a New Take on Retrieval-Augmented Gene<br>ration | [ArXiv](https://arxiv.org/abs/2402.03367) |  |
| RAG | RAFT: Adapting Language Model to Domain Specific R<br>AG | [ArXiv](https://arxiv.org/abs/2403.10131) |  |
| RAG | Adaptive-RAG: Learning to Adapt Retrieval-Augmente<br>d Large Language Models through Question Complexity | [ArXiv](https://arxiv.org/abs/2403.14403) |  |
| RAG | RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Ca<br>se Study on Agriculture | [ArXiv](https://arxiv.org/abs/2401.08406) |  |
| RAG | Fine-Tuning or Retrieval? Comparing Knowledge Inje<br>ction in LLMs | [ArXiv](https://arxiv.org/pdf/2312.05934.pdf) |  |
| Quantization, Scaling | SliceGPT: Compress Large Language Models by Deleti<br>ng Rows and Columns | [ArXiv](https://arxiv.org/pdf/2401.15024) |  |
| Prompting, Survey | A Systematic Survey of Prompt Engineering in Large<br> Language Models: Techniques and Applications | [ArXiv](https://arxiv.org/pdf/2402.07927) |  |
| Prompting, Robot, Zero-shot | Zero-Shot Task Generalization with Multi-Task Deep<br> Reinforcement Learning | [ArXiv](https://arxiv.org/pdf/1706.05064) |  |
| Prompting | Contrastive Chain-of-Thought Prompting | [ArXiv](https://arxiv.org/pdf/2311.09277) |  |
| PersonalCitation, Robot | Text2Motion: From Natural Language Instructions to<br> Feasible Plans | [ArXiv](https://arxiv.org/abs/2303.12153) |  |
| Perception, Video, Vision | CLIP4Clip: An Empirical Study of CLIP for End to E<br>nd Video Clip Retrieval | [ArXiv](https://arxiv.org/abs/2104.08860) |  |
| Perception, Task-Decompose | DoReMi: Grounding Language Model by Detecting and <br>Recovering from Plan-Execution Misalignment | [ArXiv](https://arxiv.org/abs/2307.00329) | 2023/07/01 |
| Perception, Robot, Segmentation | Language Segment-Anything |  |  |
| Perception, Robot | LiDAR-LLM: Exploring the Potential of Large Langua<br>ge Models for 3D LiDAR Understanding | [ArXiv](https://arxiv.org/abs/2312.14074) | 2023/12/21 |
| Perception, Reasoning, Robot | Reasoning Grasping via Multimodal Large Language M<br>odel | [ArXiv](https://arxiv.org/abs/2402.06798) |  |
| Perception, Reasoning | DetGPT: Detect What You Need via Reasoning | [ArXiv](https://arxiv.org/abs/2305.14167) |  |
| Perception, Reasoning | Lenna: Language Enhanced Reasoning Detection Assis<br>tant | [ArXiv](https://arxiv.org/abs/2312.02433) |  |
| Perception | Simple Open-Vocabulary Object Detection with Visio<br>n Transformers | [ArXiv](https://arxiv.org/abs/2205.06230) | 2022/05/12 |
| Perception | Grounded Language-Image Pre-training | [ArXiv](https://arxiv.org/abs/2112.03857) | 2021/12/07 |
| Perception | Grounding DINO: Marrying DINO with Grounded Pre-Tr<br>aining for Open-Set Object Detection | [ArXiv](https://arxiv.org/abs/2303.05499) | 2023/03/09 |
| Perception | PointCLIP: Point Cloud Understanding by CLIP | [ArXiv](https://arxiv.org/abs/2112.02413) | 2021/12/04 |
| Perception | DINO: DETR with Improved DeNoising Anchor Boxes fo<br>r End-to-End Object Detection | [ArXiv](https://arxiv.org/pdf/2203.03605) |  |
| Perception | Recognize Anything: A Strong Image Tagging Model | [ArXiv](https://arxiv.org/abs/2306.03514) |  |
| Perception | Simple Open-Vocabulary Object Detection with Visio<br>n Transformers | [ArXiv](https://arxiv.org/abs/2205.06230) |  |
| Perception | Sigmoid Loss for Language Image Pre-Training | [ArXiv](https://arxiv.org/abs/2303.15343) |  |
| Package | LlamaIndex | [GitHub](https://github.com/run-llama/llama_index) |  |
| Package | LangChain | [GitHub](https://github.com/langchain-ai/langchain) |  |
| Package | h2oGPT | [GitHub](https://github.com/h2oai/h2ogpt) |  |
| Package | Dify | [GitHub](https://github.com/langgenius/dify) |  |
| Package | Alpaca-LoRA | [GitHub](https://github.com/tloen/alpaca-lora) |  |
| Package | Promptlayer | [GitHub](https://promptlayer.com/) |  |
| Package | unsloth | [GitHub](https://github.com/unslothai/unsloth) |  |
| Package | Instructor: Structured LLM Outputs | [GitHub](https://github.com/jxnl/instructor) |  |
| PRM | Let's Verify Step by Step | [ArXiv](https://arxiv.org/abs/2305.20050) |  |
| PRM | Let's reward step by step: Step-Level reward model<br> as the Navigators for Reasoning | [ArXiv](https://arxiv.org/abs/2310.10080) |  |
| PPO, RLHF, Reinforcement-Learning | Secrets of RLHF in Large Language Models Part I: P<br>PO | [ArXiv](https://arxiv.org/abs/2402.01030) | 2024/02/01 |
| Open-source, VLM | OpenFlamingo: An Open-Source Framework for Trainin<br>g Large Autoregressive Vision-Language Models | [ArXiv](https://arxiv.org/abs/2308.01390) | 2023/08/02 |
| Open-source, SLM | RecurrentGemma: Moving Past Transformers for Effic<br>ient Open Language Models | [ArXiv](https://arxiv.org/abs/2404.07839) |  |
| Open-source, Perception | Grounding DINO: Marrying DINO with Grounded Pre-Tr<br>aining for Open-Set Object Detection | [ArXiv](https://arxiv.org/pdf/2303.05499) |  |
| Open-source | Gemma: Introducing new state-of-the-art open model<br>s | [ArXiv](https://blog.google/technology/developers/gemma-open-models/) |  |
| Open-source | Mistral 7B | [ArXiv](https://arxiv.org/abs/2310.06825) |  |
| Open-source | Qwen Technical Report | [ArXiv](https://arxiv.org/pdf/2309.16609) |  |
| Navigation, Reasoning, Vision | NavGPT: Explicit Reasoning in Vision-and-Language <br>Navigation with Large Language Models | [ArXiv](https://arxiv.org/pdf/2305.16986) |  |
| Natural-Language-as-Polices, Robot | RT-H: Action Hierarchies Using Language | [ArXiv](https://arxiv.org/abs/2403.01823) |  |
| Multimodal, Robot, VLM | Open-World Object Manipulation using Pre-trained V<br>ision-Language Models | [ArXiv](https://arxiv.org/abs/2303.00905) | 2023/03/02 |
| Multimodal, Robot | MOMA-Force: Visual-Force Imitation for Real-World <br>Mobile Manipulation | [ArXiv](https://arxiv.org/abs/2308.03624) | 2023/08/07 |
| Multimodal, Robot | Flamingo: a Visual Language Model for Few-Shot Lea<br>rning | [ArXiv](https://arxiv.org/abs/2204.14198) | 2022/04/29 |
| Multi-Images, VLM | Mantis: Multi-Image Instruction Tuning | [ArXiv](https://tiger-ai-lab.github.io/Blog/mantis), [GitHub](https://tiger-ai-lab.github.io/Blog/mantis) |  |
| MoE | Switch Transformers: Scaling to Trillion Parameter<br> Models with Simple and Efficient Sparsity | [ArXiv](https://arxiv.org/abs/2101.03961) |  |
| MoE | Sparse MoE as the New Dropout: Scaling Dense and S<br>elf-Slimmable Transformers | [ArXiv](https://arxiv.org/abs/2303.01610) |  |
| Mixtral, MoE | Mixtral of Experts | [ArXiv](https://arxiv.org/pdf/2401.04088) |  |
| Memory, Robot | LLM as A Robotic Brain: Unifying Egocentric Memory<br> and Control | [ArXiv](https://arxiv.org/abs/2304.09349) | 2023/04/19 |
| Memory, Reinforcement-Learning | Semantic HELM: A Human-Readable Memory for Reinfor<br>cement Learning |  |  |
| Math, Reasoning | DeepSeekMath: Pushing the Limits of Mathematical R<br>easoning in Open Language Models | [ArXiv](https://arxiv.org/pdf/2402.03300), [GitHub](https://github.com/deepseek-ai/DeepSeek-Math) |  |
| Math, PRM | Math-Shepherd: Verify and Reinforce LLMs Step-by-s<br>tep without Human Annotations | [ArXiv](https://arxiv.org/abs/2312.08935) |  |
| Math | WizardMath: Empowering Mathematical Reasoning for <br>Large Language Models via Reinforced Evol-Instruct | [ArXiv](https://arxiv.org/pdf/2308.09583) |  |
| Math | Llemma: An Open Language Model For Mathematics | [ArXiv](https://arxiv.org/pdf/2310.10631) |  |
| Low-level-action, Robot | SayTap: Language to Quadrupedal Locomotion | [ArXiv](https://arxiv.org/abs/2306.07580) | 2023/06/13 |
| Low-level-action, Robot | Prompt a Robot to Walk with Large Language Models | [ArXiv](https://arxiv.org/abs/2309.09969) | 2023/09/18 |
| LoRA, Scaling | LoRA: Low-Rank Adaptation of Large Language Models | [ArXiv](https://arxiv.org/pdf/2106.09685) |  |
| LoRA, Scaling | Vera: A General-Purpose Plausibility Estimation Mo<br>del for Commonsense Statements | [ArXiv](https://arxiv.org/abs/2305.03695) |  |
| LoRA | LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A<br> Technical Report | [ArXiv](https://arxiv.org/abs/2405.00732) |  |
| Lab | Tencent AI Lab - AppAgent, WebVoyager |  |  |
| Lab | DeepWisdom - MetaGPT |  |  |
| Lab | Reworkd AI - AgentGPT |  |  |
| Lab | OpenBMB - ChatDev, XAgent, AgentVerse |  |  |
| Lab | XLANG NLP Lab - OpenAgents |  |  |
| Lab | Rutgers University, AGI Research - OpenAGI |  |  |
| Lab | Knowledge Engineering Group (KEG) & Data Mining at<br> Tsinghua University -  CogVLM |  |  |
| Lab | OpenGVLab | [GitHub](https://github.com/OpenGVLab) |  |
| Lab | Imperial College London - Zeroshot trajectory |  |  |
| Lab | sensetime |  |  |
| Lab | tsinghua |  |  |
| Lab | Fudan NLP Group |  |  |
| Lab | Penn State University |  |  |
| LLaVA, VLM | TinyLLaVA: A Framework of Small-scale Large Multim<br>odal Models | [ArXiv](https://arxiv.org/abs/2402.14289) |  |
| LLaVA, MoE, VLM | MoE-LLaVA: Mixture of Experts for Large Vision-Lan<br>guage Models |  |  |
| LLaMA, Lightweight, Open-source | MobiLlama: Towards Accurate and Lightweight Fully <br>Transparent GPT |  |  |
| LLM, Zero-shot | GPT4Vis: What Can GPT-4 Do for Zero-shot Visual Re<br>cognition? | [ArXiv](https://arxiv.org/abs/2311.15732) | 2023/11/27 |
| LLM, Temporal Logics | NL2TL: Transforming Natural Languages to Temporal <br>Logics using Large Language Models | [ArXiv](https://arxiv.org/abs/2305.07766) | 2023/05/12 |
| LLM, Survey | A Survey of Large Language Models | [ArXiv](https://arxiv.org/abs/2303.18223) | 2023/03/31 |
| LLM, Spacial | Can Large Language Models be Good Path Planners? A<br> Benchmark and Investigation on Spatial-temporal Reasoning | [ArXiv](https://arxiv.org/abs/2310.03249) | 2023/10/05 |
| LLM, Scaling | BitNet: Scaling 1-bit Transformers for Large Langu<br>age Models | [ArXiv](https://arxiv.org/abs/2310.11453) |  |
| LLM, Robot, Task-Decompose | Do As I Can, Not As I Say: Grounding Language in R<br>obotic Affordances | [ArXiv](https://arxiv.org/abs/2204.01691) | 2022/04/04 |
| LLM, Robot, Survey | Large Language Models for Robotics: A Survey | [ArXiv](https://arxiv.org/abs/2311.07226) |  |
| LLM, Reasoning, Survey | Towards Reasoning in Large Language Models: A Surv<br>ey | [ArXiv](https://arxiv.org/abs/2212.10403) | 2022/12/20 |
| LLM, Quantization | The Era of 1-bit LLMs: All Large Language Models a<br>re in 1.58 Bits | [ArXiv](https://arxiv.org/abs/2402.17764) |  |
| LLM, PersonalCitation, Robot, Zero-shot | Language Models as Zero-Shot Trajectory Generators | [ArXiv](https://arxiv.org/abs/2310.11604) |  |
| LLM, PersonalCitation, Robot | Tree-Planner: Efficient Close-loop Task Planning w<br>ith Large Language Models01 |  |  |
| LLM, Open-source | A self-hosted, offline, ChatGPT-like chatbot, powe<br>red by Llama 2. 100% private, with no data leaving your device. | [GitHub](https://github.com/getumbrel/llama-gpt) |  |
| LLM, Open-source | OpenFlamingo: An Open-Source Framework for Trainin<br>g Large Autoregressive Vision-Language Models | [ArXiv](https://arxiv.org/abs/2308.01390) | 2023/08/02 |
| LLM, Open-source | InstructBLIP: Towards General-purpose Vision-Langu<br>age Models with Instruction Tuning | [ArXiv](https://arxiv.org/abs/2305.06500) | 2023/05/11 |
| LLM, Open-source | ChatBridge: Bridging Modalities with Large Languag<br>e Model as a Language Catalyst | [ArXiv](https://arxiv.org/abs/2305.16103) | 2023/05/25 |
| LLM, Memory | MemoryBank: Enhancing Large Language Models with L<br>ong-Term Memory | [ArXiv](https://arxiv.org/abs/2305.10250) | 2023/05/17 |
| LLM, Leaderboard | LMSYS Chatbot Arena Leaderboard |  |  |
| LLM | Language Models are Few-Shot Learners | [ArXiv](https://arxiv.org/abs/2005.14165) | 2020/05/28 |
| Intaractive, OpenGVLab, VLM | InternGPT: Solving Vision-Centric Tasks by Interac<br>ting with ChatGPT Beyond Language | [ArXiv](https://arxiv.org/abs/2305.05662) | 2023/05/09 |
| Instruction-Turning, Survey | Is Prompt All You Need? No. A Comprehensive and Br<br>oader View of Instruction Learning |  |  |
| Instruction-Turning, Survey | Vision-Language Instruction Tuning: A Review and A<br>nalysis | [ArXiv](https://arxiv.org/pdf/2311.08172) |  |
| Instruction-Turning, Survey | A Closer Look at the Limitations of Instruction Tu<br>ning | [ArXiv](https://arxiv.org/abs/2402.05119) |  |
| Instruction-Turning, Survey | A Survey on Data Selection for LLM Instruction Tun<br>ing | [ArXiv](https://arxiv.org/pdf/2402.05123) |  |
| Instruction-Turning, Self | Self-Instruct: Aligning Language Models with Self-<br>Generated Instructions | [ArXiv](https://arxiv.org/pdf/2212.10560) |  |
| Instruction-Turning, LLM, Zero-shot | Finetuned Language Models Are Zero-Shot Learners | [ArXiv](https://arxiv.org/abs/2109.01652) | 2021/09/03 |
| Instruction-Turning, LLM, Survey | Instruction Tuning for Large Language Models: A Su<br>rvey |  |  |
| Instruction-Turning, LLM, PEFT | Visual Instruction Tuning | [ArXiv](https://arxiv.org/abs/2304.08485) | 2023/04/17 |
| Instruction-Turning, LLM, PEFT | LLaMA-Adapter: Efficient Fine-tuning of Language M<br>odels with Zero-init Attention | [ArXiv](https://arxiv.org/abs/2303.16199) | 2023/03/28 |
| Instruction-Turning, LLM | Training language models to follow instructions wi<br>th human feedback | [ArXiv](https://arxiv.org/abs/2203.02155) | 2022/03/04 |
| Instruction-Turning, LLM | MiniGPT-4: Enhancing Vision-Language Understanding<br> with Advanced Large Language Models | [ArXiv](https://arxiv.org/abs/2304.10592) | 2023/04/20 |
| Instruction-Turning, LLM | Self-Instruct: Aligning Language Models with Self-<br>Generated Instructions | [ArXiv](https://arxiv.org/abs/2212.10560) | 2022/12/20 |
| Instruction-Turning | A Closer Look at the Limitations of Instruction Tu<br>ning |  |  |
| Instruction-Turning | Exploring Format Consistency for Instruction Tunin<br>g |  |  |
| Instruction-Turning | Exploring the Benefits of Training Expert Language<br> Models over Instruction Tuning | [ArXiv](https://arxiv.org/abs/2302.03202) | 2023/02/06 |
| Instruction-Turning | Tuna: Instruction Tuning using Feedback from Large<br> Language Models | [ArXiv](https://arxiv.org/abs/2310.13385) | 2023/03/06 |
| In-Context-Learning, Vision | What Makes Good Examples for Visual In-Context Lea<br>rning? |  |  |
| In-Context-Learning, Vision | Visual Prompting via Image Inpainting | [ArXiv](https://arxiv.org/abs/2209.00647) |  |
| In-Context-Learning, Video | Prompting Visual-Language Models for Efficient Vid<br>eo Understanding |  |  |
| In-Context-Learning, VQA | VisualCOMET: Reasoning about the Dynamic Context o<br>f a Still Image | [ArXiv](https://arxiv.org/abs/2004.10796) | 2020/04/22 |
| In-Context-Learning, VQA | SINC: Self-Supervised In-Context Learning for Visi<br>on-Language Tasks | [ArXiv](https://arxiv.org/abs/2307.07742) | 2023/07/15 |
| In-Context-Learning, Survey | A Survey on In-context Learning | [ArXiv](https://arxiv.org/abs/2301.00234) |  |
| In-Context-Learning, Scaling | Structured Prompting: Scaling In-Context Learning <br>to 1,000 Examples | [ArXiv](https://arxiv.org/pdf/2212.06713) | 2020/03/06 |
| In-Context-Learning, Scaling | Rethinking the Role of Scale for In-Context Learni<br>ng: An Interpretability-based Case Study at 66 Billion Scale | [ArXiv](https://arxiv.org/pdf/2212.09095) | 2022/03/06 |
| In-Context-Learning, Reinforcement-Learning | AMAGO: Scalable In-Context Reinforcement Learning <br>for Adaptive Agents | [ArXiv](https://arxiv.org/pdf/2310.09971) |  |
| In-Context-Learning, Prompt-Tuning | Visual Prompt Tuning | [ArXiv](https://arxiv.org/pdf/2203.12119) |  |
| In-Context-Learning, Perception, Vision | Visual In-Context Prompting | [ArXiv](https://arxiv.org/pdf/2311.13601) |  |
| In-Context-Learning, Many-Shot, Reasoning | Many-Shot In-Context Learning | [ArXiv](https://arxiv.org/abs/2404.11018) |  |
| In-Context-Learning, Instruction-Turning | In-Context Instruction Learning |  |  |
| In-Context-Learning | ReAct: Synergizing Reasoning and Acting in Languag<br>e Models | [ArXiv](https://arxiv.org/abs/2303.11366) | 2023/03/20 |
| In-Context-Learning | Small Models are Valuable Plug-ins for Large Langu<br>age Models | [ArXiv](https://arxiv.org/abs/2305.08848) | 2023/05/15 |
| In-Context-Learning | Generative Agents: Interactive Simulacra of Human <br>Behavior | [ArXiv](https://arxiv.org/abs/2304.03442) | 2023/04/07 |
| In-Context-Learning | Beyond the Imitation Game: Quantifying and extrapo<br>lating the capabilities of language models | [ArXiv](https://arxiv.org/abs/2206.04615) | 2022/06/09 |
| In-Context-Learning | What does CLIP know about a red circle? Visual pro<br>mpt engineering for VLMs | [ArXiv](https://arxiv.org/pdf/2304.06712) |  |
| In-Context-Learning | Can large language models explore in-context? | [ArXiv](https://arxiv.org/pdf/2403.15371) |  |
| Image, LLaMA, Perception | LLaMA-VID: An Image is Worth 2 Tokens in Large Lan<br>guage Models | [ArXiv](https://arxiv.org/pdf/2311.17043) |  |
| Hallucination, Survey | Combating Misinformation in the Age of LLMs: Oppor<br>tunities and Challenges | [ArXiv](https://arxiv.org/abs/2311.05656) |  |
| Gym, PPO, Reinforcement-Learning, Survey | Can Language Agents Approach the Performance of RL<br>? An Empirical Study On OpenAI Gym | [ArXiv](https://arxiv.org/pdf/2312.03290) |  |
| Grounding, Reinforcement-Learning | Grounding Large Language Models in Interactive Env<br>ironments with Online Reinforcement Learning | [ArXiv](https://arxiv.org/pdf/2302.02662) |  |
| Grounding, Reasoning | Visually Grounded Reasoning across Languages and C<br>ultures | [ArXiv](https://arxiv.org/pdf/2109.13238) |  |
| Grounding | V-IRL: Grounding Virtual Intelligence in Real Life |  |  |
| Google, Grounding | GLaMM: Pixel Grounding Large Multimodal Model | [ArXiv](https://arxiv.org/pdf/2311.03356) |  |
| Generation, Survey | Advances in 3D Generation: A Survey |  |  |
| Generation, Robot, Zero-shot | Zero-Shot Robotic Manipulation with Pretrained Ima<br>ge-Editing Diffusion Models | [ArXiv](https://arxiv.org/abs/2310.10639) |  |
| Generation, Robot, Zero-shot | Towards Generalizable Zero-Shot Manipulationvia Tr<br>anslating Human Interaction Plans |  |  |
| GPT4V, Robot, VLM | Closed-Loop Open-Vocabulary Mobile Manipulation wi<br>th GPT-4V | [ArXiv](https://arxiv.org/pdf/2404.10220) |  |
| GPT4, LLM | GPT-4 Technical Report | [ArXiv](https://arxiv.org/abs/2303.08774) | 2023/03/15 |
| GPT4, Instruction-Turning | INSTRUCTION TUNING WITH GPT-4 | [ArXiv](https://arxiv.org/abs/2304.03277) |  |
| GPT4, Gemini, LLM | Gemini vs GPT-4V: A Preliminary Comparison and Com<br>bination of Vision-Language Models Through Qualitative Cases | [ArXiv](https://arxiv.org/abs/2312.15011) | 2023/12/22 |
| Foundation, Robot, Survey | Foundation Models in Robotics: Applications, Chall<br>enges, and the Future | [ArXiv](https://arxiv.org/abs/2312.07843) | 2023/12/13 |
| Foundation, LLaMA, Vision | VisionLLaMA: A Unified LLaMA Interface for Vision <br>Tasks | [ArXiv](https://arxiv.org/pdf/2403.00522) |  |
| Foundation, LLM, Open-source | Code Llama: Open Foundation Models for Code | [ArXiv](https://arxiv.org/pdf/2308.12950) |  |
| Foundation, LLM, Open-source | LLaMA: Open and Efficient Foundation Language Mode<br>ls | [ArXiv](https://arxiv.org/abs/2302.13971) | 2023/02/27 |
| Feedback, Robot | Correcting Robot Plans with Natural Language Feedb<br>ack | [ArXiv](https://arxiv.org/abs/2204.05186) |  |
| Feedback, Robot | Learning to Learn Faster from Human Feedback with <br>Language Model Predictive Control | [ArXiv](https://arxiv.org/pdf/2402.11450) |  |
| Feedback, Robot | REFLECT: Summarizing Robot Experiences for Failure<br> Explanation and Correction | [ArXiv](https://arxiv.org/abs/2306.15724) | 2023/06/27 |
| Feedback, In-Context-Learning, Robot | InCoRo: In-Context Learning for Robotics Control w<br>ith Feedback Loops | [ArXiv](https://arxiv.org/abs/2402.05188) |  |
| Evaluation, LLM, Survey | A Survey on Evaluation of Large Language Models | [ArXiv](https://arxiv.org/abs/2307.03109) |  |
| Evaluation | simple-evals | [GitHub](https://github.com/openai/simple-evals) |  |
| End2End, Multimodal, Robot | VIMA: General Robot Manipulation with Multimodal P<br>rompts | [ArXiv](https://arxiv.org/abs/2210.03094) | 2022/10/06 |
| End2End, Multimodal, Robot | PaLM-E: An Embodied Multimodal Language Model | [ArXiv](https://arxiv.org/abs/2303.03378) | 2023/03/06 |
| End2End, Multimodal, Robot | Physically Grounded Vision-Language Models for Rob<br>otic Manipulation | [ArXiv](https://arxiv.org/abs/2309.02561) | 2023/09/05 |
| Enbodied | Embodied Question Answering | [ArXiv](https://arxiv.org/abs/1711.11543) |  |
| Embodied, World-model | Language Models Meet World Models: Embodied Experi<br>ences Enhance Language Models |  |  |
| Embodied, Robot, Task-Decompose | Embodied Task Planning with Large Language Models | [ArXiv](https://arxiv.org/abs/2307.01848) | 2023/07/04 |
| Embodied, Robot | Large Language Models as Generalizable Policies fo<br>r Embodied Tasks | [ArXiv](https://arxiv.org/pdf/2310.17722) |  |
| Embodied, Reasoning, Robot | Natural Language as Policies: Reasoning for Coordi<br>nate-Level Embodied Control with LLMs | [ArXiv](https://arxiv.org/abs/2403.13801), [GitHub](https://github.com/shure-dev/NLaP) | 2024/03/20 |
| Embodied, LLM, Robot, Survey | The Development of LLMs for Embodied Navigation | [ArXiv](https://arxiv.org/abs/2311.00530) | 2023/11/01 |
| Driving, Spacial | GPT-Driver: Learning to Drive with GPT | [ArXiv](https://arxiv.org/abs/2310.01415) | 2023/10/02 |
| Drive, Survey | A Survey on Multimodal Large Language Models for A<br>utonomous Driving | [ArXiv](https://arxiv.org/abs/2311.12320) |  |
| Distilling, Survey | A Survey on Knowledge Distillation of Large Langua<br>ge Models |  |  |
| Distilling | Distilling Step-by-Step! Outperforming Larger Lang<br>uage Models with Less Training Data and Smaller Model Sizes01 | [ArXiv](https://arxiv.org/pdf/2305.02301) |  |
| Diffusion, Text-to-Image | Mastering Text-to-Image Diffusion: Recaptioning, P<br>lanning, and Generating with Multimodal LLMs | [ArXiv](https://arxiv.org/pdf/2401.11708) |  |
| Diffusion, Survey | On the Design Fundamentals of Diffusion Models: A <br>Survey | [ArXiv](https://arxiv.org/abs/2306.04542) |  |
| Diffusion, Robot | 3D Diffusion Policy | [ArXiv](https://arxiv.org/abs/2403.03954) |  |
| Diffusion | A latent text-to-image diffusion model |  |  |
| Demonstration, GPT4, PersonalCitation, Robot, VLM | GPT-4V(ision) for Robotics: Multimodal Task Planni<br>ng from Human Demonstration |  |  |
| Datatset, LLM, Survey | A Survey on Data Selection for Language Models | [ArXiv](https://arxiv.org/pdf/2402.05123) |  |
| Datatset, Instruction-Turning | REVO-LION: EVALUATING AND REFINING VISION LANGUAGE<br> INSTRUCTION TUNING DATASETS |  |  |
| Datatset, Instruction-Turning | Synthetic Data (Almost) from Scratch: Generalized <br>Instruction Tuning for Language Models |  |  |
| Datatset | PRM800K: A Process Supervision Dataset | [GitHub](https://github.com/openai/prm800k) |  |
| Data-generation, Robot | GenSim: Generating Robotic Simulation Tasks via La<br>rge Language Models | [ArXiv](https://arxiv.org/abs/2310.01361) | 2023/10/02 |
| Data-generation, Robot | RoboGen: Towards Unleashing Infinite Data for Auto<br>mated Robot Learning via Generative Simulation | [ArXiv](https://arxiv.org/abs/2311.01455) | 2023/11/02 |
| DPO, PPO, RLHF | A Comprehensive Survey of LLM Alignment Techniques<br>: RLHF, RLAIF, PPO, DPO and More | [ArXiv](https://arxiv.org/pdf/2407.16216) |  |
| DPO | Is DPO Superior to PPO for LLM Alignment? A Compre<br>hensive Study | [ArXiv](https://arxiv.org/abs/2404.10719) |  |
| Context-Window, Scaling | Infini-gram: Scaling Unbounded n-gram Language Mod<br>els to a Trillion Tokens |  |  |
| Context-Window, Scaling | LONGNET: Scaling Transformers to 1,000,000,000 Tok<br>ens | [ArXiv](https://arxiv.org/abs/2307.02486) | 2023/07/01 |
| Context-Window, Reasoning, RoPE, Scaling | Resonance RoPE: Improving Context Length Generaliz<br>ation of Large Language Models | [ArXiv](https://arxiv.org/pdf/2403.00071) |  |
| Context-Window, LLM, RoPE, Scaling | LongRoPE: Extending LLM Context Window Beyond 2 Mi<br>llion Tokens | [ArXiv](https://arxiv.org/abs/2402.13753) |  |
| Context-Window, Foundation, Gemini, LLM, Scaling | Gemini 1.5: Unlocking multimodal understanding acr<br>oss millions of tokens of context |  |  |
| Context-Window, Foundation | Mamba: Linear-Time Sequence Modeling with Selectiv<br>e State Spaces | [ArXiv](https://arxiv.org/pdf/2312.00752) |  |
| Context-Window | RoFormer: Enhanced Transformer with Rotary Positio<br>n Embedding | [ArXiv](https://arxiv.org/pdf/2309.09969) |  |
| Context-Awere, Context-Window | DynaCon: Dynamic Robot Planner with Contextual Awa<br>reness via LLMs | [ArXiv](https://arxiv.org/pdf/2309.16031) |  |
| Computer-Resource, Scaling | FlashAttention: Fast and Memory-Efficient Exact At<br>tention with IO-Awareness | [ArXiv](https://arxiv.org/pdf/2205.14135.pdf) |  |
| Compress, Scaling | (Long)LLMLingua: Enhancing Large Language Model In<br>ference via Prompt Compression | [ArXiv](https://arxiv.org/abs/2310.05736) |  |
| Compress, Quantization, Survey | A Survey on Model Compression for Large Language M<br>odels | [ArXiv](https://arxiv.org/abs/2308.07633) |  |
| Compress, Prompting | Learning to Compress Prompts with Gist Tokens | [ArXiv](https://arxiv.org/abs/2304.08467) |  |
| Code-as-Policies, VLM, VQA | Visual Programming: Compositional visual reasoning<br> without training | [ArXiv](https://arxiv.org/abs/2211.11559) | 2022/11/18 |
| Code-as-Policies, Robot | SMART-LLM: Smart Multi-Agent Robot Task Planning u<br>sing Large Language Models | [ArXiv](https://arxiv.org/abs/2309.10062) | 2023/09/18 |
| Code-as-Policies, Robot | RoboScript: Code Generation for Free-Form Manipula<br>tion Tasks across Real and Simulation | [ArXiv](https://arxiv.org/pdf/2402.14623) |  |
| Code-as-Policies, Robot | Creative Robot Tool Use with Large Language Models | [ArXiv](https://arxiv.org/pdf/2310.13065) |  |
| Code-as-Policies, Reinforcement-Learning, Reward | Code as Reward: Empowering Reinforcement Learning <br>with VLMs | [ArXiv](https://arxiv.org/abs/2402.04764) |  |
| Code-as-Policies, Reasoning, VLM, VQA | ViperGPT: Visual Inference via Python Execution fo<br>r Reasoning | [ArXiv](https://arxiv.org/abs/2303.08128) | 2023/03/14 |
| Code-as-Policies, Reasoning | Chain of Code: Reasoning with a Language Model-Aug<br>mented Code Emulator | [ArXiv](https://arxiv.org/pdf/2312.04474) |  |
| Code-as-Policies, PersonalCitation, Robot, Zero-shot | Socratic Models: Composing Zero-Shot Multimodal Re<br>asoning with Language | [ArXiv](https://arxiv.org/abs/2204.00598) | 2022/04/01 |
| Code-as-Policies, PersonalCitation, Robot, State-Manage | Statler: State-Maintaining Language Models for Emb<br>odied Reasoning | [ArXiv](https://arxiv.org/abs/2306.17840) | 2023/06/30 |
| Code-as-Policies, PersonalCitation, Robot | ProgPrompt: Generating Situated Robot Task Plans u<br>sing Large Language Models | [ArXiv](https://arxiv.org/abs/2209.11302) | 2022/09/22 |
| Code-as-Policies, PersonalCitation, Robot | RoboCodeX:Multi-modal Code Generation forRobotic B<br>ehavior Synthesis | [ArXiv](https://arxiv.org/abs/2402.16117) |  |
| Code-as-Policies, PersonalCitation, Robot | RoboGPT: an intelligent agent of making embodied l<br>ong-term decisions for daily instruction tasks |  |  |
| Code-as-Policies, PersonalCitation, Robot | ChatGPT for Robotics: Design Principles and Model <br>Abilities |  |  |
| Code-as-Policies, Multimodal, OpenGVLab, PersonalCitation, Robot | Instruct2Act: Mapping Multi-modality Instructions <br>to Robotic Actions with Large Language Model | [ArXiv](https://arxiv.org/abs/2305.11176) | 2023/05/18 |
| Code-as-Policies, Embodied, PersonalCitation, Robot | Code as Policies: Language Model Programs for Embo<br>died Control | [ArXiv](https://arxiv.org/abs/2209.07753) | 2022/09/16 |
| Code-as-Policies, Embodied, PersonalCitation, Reasoning, Robot, Task-Decompose | Inner Monologue: Embodied Reasoning through Planni<br>ng with Language Models | [ArXiv](https://arxiv.org/abs/2207.05608) |  |
| Code-LLM, Front-End | Design2Code: How Far Are We From Automating Front-<br>End Engineering? | [ArXiv](https://arxiv.org/pdf/2403.03163) |  |
| Code-LLM | StarCoder 2 and The Stack v2: The Next Generation |  |  |
| Chain-of-Thought, Reasoning, Table | Chain-of-table: Evolving tables in the reasoning c<br>hain for table understanding | [ArXiv](https://arxiv.org/pdf/2401.04398) |  |
| Chain-of-Thought, Reasoning, Survey | Towards Understanding Chain-of-Thought Prompting: <br>An Empirical Study of What Matters | [ArXiv](https://arxiv.org/abs/2212.10001) | 2023/12/20 |
| Chain-of-Thought, Reasoning, Survey | A Survey of Chain of Thought Reasoning: Advances, <br>Frontiers and Future | [ArXiv](https://arxiv.org/abs/2309.15402) | 2023/09/27 |
| Chain-of-Thought, Reasoning | Chain-of-Thought Prompting Elicits Reasoning in La<br>rge Language Models | [ArXiv](https://arxiv.org/abs/2201.11903) | 2022/01/28 |
| Chain-of-Thought, Reasoning | Tree of Thoughts: Deliberate Problem Solving with <br>Large Language Models | [ArXiv](https://arxiv.org/abs/2305.10601) | 2023/05/17 |
| Chain-of-Thought, Reasoning | Multimodal Chain-of-Thought Reasoning in Language <br>Models | [ArXiv](https://arxiv.org/abs/2302.00923) | 2023/02/02 |
| Chain-of-Thought, Reasoning | Verify-and-Edit: A Knowledge-Enhanced Chain-of-Tho<br>ught Framework | [ArXiv](https://arxiv.org/abs/2305.03268) | 2023/05/05 |
| Chain-of-Thought, Reasoning | Skeleton-of-Thought: Large Language Models Can Do <br>Parallel Decoding | [ArXiv](https://arxiv.org/abs/2307.15337) | 2023/07/28 |
| Chain-of-Thought, Reasoning | Rethinking with Retrieval: Faithful Large Language<br> Model Inference | [ArXiv](https://arxiv.org/abs/2301.00303) | 2022/12/31 |
| Chain-of-Thought, Reasoning | Self-Consistency Improves Chain of Thought Reasoni<br>ng in Language Models | [ArXiv](https://arxiv.org/abs/2203.11171) | 2022/03/21 |
| Chain-of-Thought, Reasoning | Chain-of-Thought Hub: A Continuous Effort to Measu<br>re Large Language Models' Reasoning Performance | [ArXiv](https://arxiv.org/abs/2305.17306) | 2023/05/26 |
| Chain-of-Thought, Reasoning | Skeleton-of-Thought: Prompting LLMs for Efficient <br>Parallel Generation | [ArXiv](https://arxiv.org/pdf/2307.15337) |  |
| Chain-of-Thought, Prompting | Chain-of-Thought Reasoning Without Prompting | [ArXiv](https://arxiv.org/pdf/2402.10200) |  |
| Chain-of-Thought, Planning, Reasoning | SelfCheck: Using LLMs to Zero-Shot Check Their Own<br> Step-by-Step Reasoning | [ArXiv](https://arxiv.org/abs/2308.00436) | 2023/08/01 |
| Chain-of-Thought, In-Context-Learning, Self | Measuring and Narrowing the Compositionality Gap i<br>n Language Models | [ArXiv](https://arxiv.org/abs/2210.03350) | 2022/10/07 |
| Chain-of-Thought, In-Context-Learning, Self | Self-Polish: Enhance Reasoning in Large Language M<br>odels via Problem Refinement | [ArXiv](https://arxiv.org/abs/2305.14497) | 2023/05/23 |
| Chain-of-Thought, In-Context-Learning | Chain-of-Table: Evolving Tables in the Reasoning C<br>hain for Table Understanding | [ArXiv](https://arxiv.org/abs/2401.04398) |  |
| Chain-of-Thought, In-Context-Learning | Self-Refine: Iterative Refinement with Self-Feedba<br>ck | [ArXiv](https://arxiv.org/abs/2303.17651) | 2023/03/30 |
| Chain-of-Thought, In-Context-Learning | Plan-and-Solve Prompting: Improving Zero-Shot Chai<br>n-of-Thought Reasoning by Large Language Models | [ArXiv](https://arxiv.org/abs/2305.04091) | 2023/05/06 |
| Chain-of-Thought, In-Context-Learning | PAL: Program-aided Language Models | [ArXiv](https://arxiv.org/abs/2211.10435) | 2022/11/18 |
| Chain-of-Thought, In-Context-Learning | Reasoning with Language Model is Planning with Wor<br>ld Model | [ArXiv](https://arxiv.org/abs/2305.14992) | 2023/05/24 |
| Chain-of-Thought, In-Context-Learning | Least-to-Most Prompting Enables Complex Reasoning <br>in Large Language Models | [ArXiv](https://arxiv.org/abs/2205.10625) | 2022/05/21 |
| Chain-of-Thought, In-Context-Learning | Complexity-Based Prompting for Multi-Step Reasonin<br>g | [ArXiv](https://arxiv.org/abs/2210.00720) | 2022/10/03 |
| Chain-of-Thought, In-Context-Learning | Maieutic Prompting: Logically Consistent Reasoning<br> with Recursive Explanations | [ArXiv](https://arxiv.org/abs/2205.11822) | 2022/05/24 |
| Chain-of-Thought, In-Context-Learning | Algorithm of Thoughts: Enhancing Exploration of Id<br>eas in Large Language Models | [ArXiv](https://arxiv.org/abs/2308.10379) | 2023/08/20 |
| Chain-of-Thought, GPT4, Reasoning, Robot | Look Before You Leap: Unveiling the Power ofGPT-4V<br> in Robotic Vision-Language Planning | [ArXiv](https://robot-vila.github.io/ViLa.pdf) | 2023/11/29 |
| Chain-of-Thought, Embodied, Robot | EgoCOT: Embodied Chain-of-Thought Dataset for Visi<br>on Language Pre-training | [ArXiv](https://arxiv.org/pdf/2305.15021) |  |
| Chain-of-Thought, Embodied, PersonalCitation, Robot, Task-Decompose | EmbodiedGPT: Vision-Language Pre-Training via Embo<br>died Chain of Thought | [ArXiv](https://arxiv.org/abs/2305.15021) | 2023/05/24 |
| Chain-of-Thought, Code-as-Policies, PersonalCitation, Robot | Demo2Code: From Summarizing Demonstrations to Synt<br>hesizing Code via Extended Chain-of-Thought | [ArXiv](https://arxiv.org/abs/2305.16744) |  |
| Chain-of-Thought, Code-as-Policies | Chain of Code: Reasoning with a Language Model-Aug<br>mented Code Emulator | [ArXiv](https://arxiv.org/abs/2312.04474) |  |
| Caption, Video | PLLaVA : Parameter-free LLaVA Extension from Image<br>s to Videos for Video Dense Captioning | [ArXiv](https://arxiv.org/pdf/2404.16994) |  |
| Caption, VLM, VQA | Caption Anything: Interactive Image Description wi<br>th Diverse Multimodal Controls | [ArXiv](https://arxiv.org/abs/2305.02677) | 2023/05/04 |
| CRAG, RAG | Corrective Retrieval Augmented Generation | [ArXiv](https://arxiv.org/abs/2401.15884) |  |
| Brain, Instruction-Turning | Instruction-tuning Aligns LLMs to the Human Brain | [ArXiv](https://arxiv.org/pdf/2312.00575) |  |
| Brain, Conscious | Could a Large Language Model be Conscious? |  |  |
| Brain | LLM-BRAIn: AI-driven Fast Generation of Robot Beha<br>viour Tree based on Large Language Model | [ArXiv](https://arxiv.org/pdf/2305.19352) |  |
| Brain | A Neuro-Mimetic Realization of the Common Model of<br> Cognition via Hebbian Learning and Free Energy Minimization |  |  |
| Benchmark, Sora, Text-to-Video | LIDA: A Tool for Automatic Generation of Grammar-A<br>gnostic Visualizations and Infographics using Large Language Models01 |  |  |
| Benchmark, In-Context-Learning | ARB: Advanced Reasoning Benchmark for Large Langua<br>ge Models | [ArXiv](https://arxiv.org/abs/2307.13692) | 2023/07/25 |
| Benchmark, In-Context-Learning | PlanBench: An Extensible Benchmark for Evaluating <br>Large Language Models on Planning and Reasoning about Change | [ArXiv](https://arxiv.org/abs/2206.10498) | 2022/06/21 |
| Benchmark, GPT4 | Sparks of Artificial General Intelligence: Early e<br>xperiments with GPT-4 |  |  |
| Awesome Repo, VLM | awesome-vlm-architectures | [GitHub](https://github.com/gokayfem/awesome-vlm-architectures) |  |
| Awesome Repo, Survey | LLMSurvey | [GitHub](https://github.com/RUCAIBox/LLMSurvey) |  |
| Awesome Repo, Robot | Awesome-LLM-Robotics | [GitHub](https://github.com/GT-RIPL/Awesome-LLM-Robotics) |  |
| Awesome Repo, Reasoning | Awesome LLM Reasoning | [GitHub](https://github.com/atfortes/Awesome-LLM-Reasoning) |  |
| Awesome Repo, Reasoning | Awesome-Reasoning-Foundation-Models | [GitHub](https://github.com/reasoning-survey/Awesome-Reasoning-Foundation-Models) |  |
| Awesome Repo, RLHF, Reinforcement-Learning | Awesome RLHF (RL with Human Feedback) | [GitHub](https://github.com/opendilab/awesome-RLHF) |  |
| Awesome Repo, Perception, VLM | Awesome Vision-Language Navigation | [GitHub](https://github.com/daqingliu/awesome-vln) |  |
| Awesome Repo, Package | Awesome LLMOps | [GitHub](https://github.com/tensorchord/Awesome-LLMOps) |  |
| Awesome Repo, Multimodal | Awesome-Multimodal-LLM | [GitHub](https://github.com/Atomic-man007/Awesome_Multimodel_LLM) |  |
| Awesome Repo, Multimodal | Awesome-Multimodal-Large-Language-Models | [GitHub](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models) |  |
| Awesome Repo, Math, Science | Awesome Scientific Language Models | [GitHub](https://github.com/yuzhimanhua/Awesome-Scientific-Language-Models) |  |
| Awesome Repo, LLM, Vision | LLM-in-Vision | [GitHub](https://github.com/DirtyHarryLYL/LLM-in-Vision) |  |
| Awesome Repo, LLM, VLM | Multimodal & Large Language Models | [GitHub](https://github.com/Yangyi-Chen/Multimodal-AND-Large-Language-Models) |  |
| Awesome Repo, LLM, Survey | Awesome-LLM-Survey | [GitHub](https://github.com/HqWu-HITCS/Awesome-LLM-Survey) |  |
| Awesome Repo, LLM, Robot | Everything-LLMs-And-Robotics | [GitHub](https://github.com/jrin771/Everything-LLMs-And-Robotics) |  |
| Awesome Repo, LLM, Leaderboard | LLM-Leaderboard | [GitHub](https://github.com/LudwigStumpp/llm-leaderboard) |  |
| Awesome Repo, LLM | Awesome-LLM | [GitHub](https://github.com/Hannibal046/Awesome-LLM) |  |
| Awesome Repo, Korean | awesome-korean-llm | [GitHub](https://github.com/NomaDamas/awesome-korean-llm) |  |
| Awesome Repo, Japanese, LLM | 日本語LLMまとめ | [GitHub](https://github.com/llm-jp/awesome-japanese-llm) |  |
| Awesome Repo, In-Context-Learning | Paper List for In-context Learning | [GitHub](https://github.com/dqxiu/ICL_PaperList) |  |
| Awesome Repo, IROS, Robot | IROS2023PaperList | [GitHub](https://github.com/gonultasbu/IROS2023PaperList) |  |
| Awesome Repo, Hallucination, Survey | A Survey on Hallucination in Large Language Models<br>: Principles, Taxonomy, Challenges, and Open Questions | [ArXiv](https://arxiv.org/abs/2311.05232), [GitHub](https://github.com/LuckyyySTA/Awesome-LLM-hallucination) |  |
| Awesome Repo, Embodied | Awesome Embodied Vision | [GitHub](https://github.com/ChanganVR/awesome-embodied-vision) |  |
| Awesome Repo, Diffusion | Awesome-Diffusion-Models | [GitHub](https://github.com/diff-usion/Awesome-Diffusion-Models) |  |
| Awesome Repo, Compress | Awesome LLM Compression | [GitHub](https://github.com/HuangOwen/Awesome-LLM-Compression) |  |
| Awesome Repo, Chinese | Awesome-Chinese-LLM | [GitHub](https://github.com/HqWu-HITCS/Awesome-Chinese-LLM) |  |
| Awesome Repo, Chain-of-Thought | Chain-of-ThoughtsPapers | [GitHub](https://github.com/Timothyxxx/Chain-of-ThoughtsPapers) |  |
| Automate, Prompting | Large Language Models Are Human-Level Prompt Engin<br>eers | [ArXiv](https://arxiv.org/abs/2211.01910) | 2022/11/03 |
| Automate, Chain-of-Thought, Reasoning | Automatic Chain of Thought Prompting in Large Lang<br>uage Models | [ArXiv](https://arxiv.org/abs/2210.03493) | 2022/10/07 |
| Audio2Video, Diffusion, Generation, Video | EMO: Emote Portrait Alive - Generating Expressive <br>Portrait Videos with Audio2Video Diffusion Model under Weak Conditions | [ArXiv](https://arxiv.org/pdf/2402.17485) |  |
| Audio | Robust Speech Recognition via Large-Scale Weak Sup<br>ervision |  |  |
| Apple, VLM | MM1: Methods, Analysis & Insights from Multimodal <br>LLM Pre-training | [ArXiv](https://arxiv.org/pdf/2403.09611) |  |
| Apple, VLM | Guiding Instruction-based Image Editing via Multim<br>odal Large Language Models | [ArXiv](https://arxiv.org/pdf/2309.17102) |  |
| Apple, Robot | Large Language Models as Generalizable Policies fo<br>r Embodied Tasks | [ArXiv](https://arxiv.org/pdf/2310.17722) |  |
| Apple, LLM, Open-source | OpenELM: An Efficient Language Model Family with O<br>pen Training and Inference Framework | [ArXiv](https://arxiv.org/pdf/2404.14619) |  |
| Apple, LLM | ReALM: Reference Resolution As Language Modeling | [ArXiv](https://arxiv.org/pdf/2403.20329) |  |
| Apple, LLM | LLM in a flash: Efficient Large Language Model Inf<br>erence with Limited Memory | [ArXiv](https://arxiv.org/abs/2312.11514) |  |
| Apple, In-Context-Learning, Perception | SAM-CLIP: Merging Vision Foundation Models towards<br> Semantic and Spatial Understanding | [ArXiv](https://arxiv.org/pdf/2310.15308) |  |
| Apple, Code-as-Policies, Robot | Executable Code Actions Elicit Better LLM Agents | [ArXiv](https://arxiv.org/pdf/2402.01030) |  |
| Apple | Ferret-v2: An Improved Baseline for Referring and <br>Grounding with Large Language Models | [ArXiv](https://arxiv.org/pdf/2404.07973) |  |
| Apple | Ferret-UI: Grounded Mobile UI Understanding with M<br>ultimodal LLMs | [ArXiv](https://arxiv.org/pdf/2404.05719) |  |
| Apple | Ferret: Refer and Ground Anything Anywhere at Any <br>Granularity | [ArXiv](https://arxiv.org/pdf/2310.07704) |  |
| Anything, LLM, Open-source, Perception, Segmentation | Segment Anything | [ArXiv](https://arxiv.org/abs/2304.02643) | 2023/04/05 |
| Anything, Depth | Depth Anything: Unleashing the Power of Large-Scal<br>e Unlabeled Data | [ArXiv](https://arxiv.org/pdf/2401.10891) |  |
| Anything, Caption, Perception, Segmentation | Segment and Caption Anything | [ArXiv](https://arxiv.org/abs/2312.00869) |  |
| Anything, CLIP, Perception | SAM-CLIP: Merging Vision Foundation Models towards<br> Semantic and Spatial Understanding | [ArXiv](https://arxiv.org/pdf/2310.15308) |  |
| Agent-Project, Code-LLM | open-interpreter | [GitHub](https://github.com/OpenInterpreter/open-interpreter) |  |
| Agent, Web | WebLINX: Real-World Website Navigation with Multi-<br>Turn Dialogue | [ArXiv](https://arxiv.org/pdf/2402.05930) |  |
| Agent, Web | WebVoyager: Building an End-to-End Web Agent with <br>Large Multimodal Models | [ArXiv](https://arxiv.org/pdf/2401.13919) |  |
| Agent, Web | OmniACT: A Dataset and Benchmark for Enabling Mult<br>imodal Generalist Autonomous Agents for Desktop and Web | [ArXiv](https://arxiv.org/abs/2402.17553) |  |
| Agent, Web | OS-Copilot: Towards Generalist Computer Agents wit<br>h Self-Improvement | [ArXiv](https://arxiv.org/abs/2402.07456) |  |
| Agent, Video-for-Agent | Video as the New Language for Real-World Decision <br>Making |  |  |
| Agent, VLM | AssistGPT: A General Multi-modal Assistant that ca<br>n Plan, Execute, Inspect, and Learn | [ArXiv](https://arxiv.org/abs//2306.08640) |  |
| Agent, Tool | Gorilla: Large Language Model Connected with Massi<br>ve APIs | [ArXiv](https://arxiv.org/pdf/2305.15334) |  |
| Agent, Tool | ToolLLM: Facilitating Large Language Models to Mas<br>ter 16000+ Real-world APIs | [ArXiv](https://arxiv.org/pdf/2307.16789) |  |
| Agent, Survey | A Survey on Large Language Model based Autonomous <br>Agents | [ArXiv](https://arxiv.org/abs/2308.11432) | 2023/08/22 |
| Agent, Survey | The Rise and Potential of Large Language Model Bas<br>ed Agents: A Survey | [ArXiv](https://arxiv.org/abs/2309.07864) | 2023/09/14 |
| Agent, Survey | Agent AI: Surveying the Horizons of Multimodal Int<br>eraction | [ArXiv](https://arxiv.org/pdf/2401.03568) |  |
| Agent, Survey | Large Multimodal Agents: A Survey | [ArXiv](https://arxiv.org/pdf/2402.15116) |  |
| Agent, Soft-Dev | Communicative Agents for Software Development | [GitHub](https://github.com/OpenBMB/ChatDev) |  |
| Agent, Soft-Dev | MetaGPT: Meta Programming for A Multi-Agent Collab<br>orative Framework | [ArXiv](https://arxiv.org/pdf/2308.00352) |  |
| Agent, Robot, Survey | A Survey on LLM-based Autonomous Agents | [GitHub](https://github.com/Paitesanshi/LLM-Agent-Survey) |  |
| Agent, Reinforcement-Learning, Reward | Reward Design with Language Models | [ArXiv](https://arxiv.org/abs/2303.00001) | 2023/02/27 |
| Agent, Reinforcement-Learning, Reward | EAGER: Asking and Answering Questions for Automati<br>c Reward Shaping in Language-guided RL | [ArXiv](https://arxiv.org/abs/2206.09674) | 2022/06/20 |
| Agent, Reinforcement-Learning, Reward | Text2Reward: Automated Dense Reward Function Gener<br>ation for Reinforcement Learning | [ArXiv](https://arxiv.org/abs/2309.11489) | 2023/09/20 |
| Agent, Reinforcement-Learning | Eureka: Human-Level Reward Design via Coding Large<br> Language Models | [ArXiv](https://arxiv.org/abs/2310.12931) | 2023/10/19 |
| Agent, Reinforcement-Learning | Language to Rewards for Robotic Skill Synthesis | [ArXiv](https://arxiv.org/abs/2306.08647) | 2023/06/14 |
| Agent, Reinforcement-Learning | Language Instructed Reinforcement Learning for Hum<br>an-AI Coordination | [ArXiv](https://arxiv.org/abs/2304.07297) | 2023/04/13 |
| Agent, Reinforcement-Learning | Guiding Pretraining in Reinforcement Learning with<br> Large Language Models | [ArXiv](https://arxiv.org/abs/2302.06692) | 2023/02/13 |
| Agent, Reinforcement-Learning | STARLING: SELF-SUPERVISED TRAINING OF TEXTBASED RE<br>INFORCEMENT LEARNING AGENT WITH LARGE LANGUAGE MODELS |  |  |
| Agent, Reasoning, Zero-shot | Agent Instructs Large Language Models to be Genera<br>l Zero-Shot Reasoners | [ArXiv](https://arxiv.org/abs/2310.03710) | 2023/10/05 |
| Agent, Reasoning | Pangu-Agent: A Fine-Tunable Generalist Agent with <br>Structured Reasoning | [ArXiv](https://arxiv.org/pdf/2312.14878) |  |
| Agent, Reasoning | AGENT INSTRUCTS LARGE LANGUAGE MODELS TO BE GENERA<br>L ZERO-SHOT REASONERS | [ArXiv](https://arxiv.org/pdf/2310.03710) |  |
| Agent, Multimodal, Robot | A Generalist Agent | [ArXiv](https://arxiv.org/abs/2205.06175) | 2022/05/12 |
| Agent, Multi | War and Peace (WarAgent): Large Language Model-bas<br>ed Multi-Agent Simulation of World Wars | [ArXiv](https://arxiv.org/abs/2311.17227) |  |
| Agent, MobileApp | You Only Look at Screens: Multimodal Chain-of-Acti<br>on Agents | [ArXiv](https://arxiv.org/pdf/2309.11436), [GitHub](https://github.com/cooelf/Auto-UI) |  |
| Agent, Minecraft, Reinforcement-Learning | RLAdapter: Bridging Large Language Models to Reinf<br>orcement Learning in Open Worlds |  |  |
| Agent, Minecraft | Voyager: An Open-Ended Embodied Agent with Large L<br>anguage Models | [ArXiv](https://arxiv.org/abs/2305.16291) | 2023/05/25 |
| Agent, Minecraft | Describe, Explain, Plan and Select: Interactive Pl<br>anning with Large Language Models Enables Open-World Multi-Task Agents | [ArXiv](https://arxiv.org/abs/2302.01560) | 2023/02/03 |
| Agent, Minecraft | LARP: Language-Agent Role Play for Open-World Game<br>s | [ArXiv](https://arxiv.org/pdf/2312.17653) |  |
| Agent, Minecraft | Steve-Eye: Equipping LLM-based Embodied Agents wit<br>h Visual Perception in Open Worlds | [ArXiv](https://arxiv.org/pdf/2310.13255) |  |
| Agent, Minecraft | S-Agents: Self-organizing Agents in Open-ended Env<br>ironment | [ArXiv](https://arxiv.org/pdf/2402.04578) |  |
| Agent, Minecraft | Ghost in the Minecraft: Generally Capable Agents f<br>or Open-World Environments via Large Language Models with Text-based Knowledge and Memory01 | [ArXiv](https://arxiv.org/pdf/2305.17144) |  |
| Agent, Memory, RAG, Robot | RAP: Retrieval-Augmented Planning with Contextual <br>Memory for Multimodal LLM Agents | [ArXiv](https://arxiv.org/abs/2402.03610) | 2024/02/06 |
| Agent, Memory, Minecraft | JARVIS-1: Open-World Multi-task Agents with Memory<br>-Augmented Multimodal Language Models | [ArXiv](https://arxiv.org/abs/2311.05997) | 2023/11/10 |
| Agent, LLM, Planning | LLM-Planner: Few-Shot Grounded Planning for Embodi<br>ed Agents with Large Language Models | [ArXiv](https://arxiv.org/pdf/2212.04088) |  |
| Agent, Instruction-Turning | AgentTuning: Enabling Generalized Agent Abilities <br>For LLMs | [ArXiv](https://arxiv.org/abs/2310.12823) |  |
| Agent, Game | LEARNING EMBODIED VISION-LANGUAGE PRO- GRAMMING FR<br>OM INSTRUCTION, EXPLORATION, AND ENVIRONMENTAL FEEDBACK | [ArXiv](https://arxiv.org/pdf/2310.08588) |  |
| Agent, GUI, Web | "What’s important here?": Opportunities and Challe<br>nges of Using LLMs in Retrieving Informatio from Web Interfaces | [ArXiv](https://arxiv.org/pdf/2312.06147) |  |
| Agent, GUI, MobileApp | Mobile-Agent: Autonomous Multi-Modal Mobile Device<br> Agent with Visual Perception |  |  |
| Agent, GUI, MobileApp | AppAgent: Multimodal Agents as Smartphone Users | [ArXiv](https://arxiv.org/pdf/2312.13771) |  |
| Agent, GUI, MobileApp | You Only Look at Screens: Multimodal Chain-of-Acti<br>on Agents |  |  |
| Agent, GUI | CogAgent: A Visual Language Model for GUI Agents | [ArXiv](https://arxiv.org/pdf/2312.08914) |  |
| Agent, GUI | ScreenAgent: A Computer Control Agent Driven by Vi<br>sual Language Large Model | [GitHub](https://github.com/niuzaisheng/ScreenAgent) |  |
| Agent, GUI | SeeClick: Harnessing GUI Grounding for Advanced Vi<br>sual GUI Agents | [ArXiv](https://arxiv.org/pdf/2401.10935) |  |
| Agent, GPT4, Web | GPT-4V(ision) is a Generalist Web Agent, if Ground<br>ed | [ArXiv](https://arxiv.org/pdf/2401.01614) |  |
| Agent, Feedback, Reinforcement-Learning, Robot | Accelerating Reinforcement Learning of Robotic Man<br>ipulations via Feedback from Large Language Models | [ArXiv](https://arxiv.org/abs/2311.02379) | 2023/11/04 |
| Agent, Feedback, Reinforcement-Learning | AdaRefiner: Refining Decisions of Language Models <br>with Adaptive Feedback | [ArXiv](https://arxiv.org/abs/2309.17176) | 2023/09/29 |
| Agent, End2End, Game, Robot | An Interactive Agent Foundation Model | [ArXiv](https://arxiv.org/abs/2402.05929) |  |
| Agent, Embodied, Survey | Application of Pretrained Large Language Models in<br> Embodied Artificial Intelligence | [ArXiv](https://www.semanticscholar.org/paper/Application-of-Pretrained-Large-Language-Models-in-Kovalev-Panov/04f87baf7d1b3eb303a52a8a66c8189f396dd114) |  |
| Agent, Embodied, Robot | AutoRT: Embodied Foundation Models for Large Scale<br> Orchestration of Robotic Agents | [ArXiv](https://arxiv.org/abs/2401.12963) |  |
| Agent, Embodied, Robot | OPEx: A Component-Wise Analysis of LLM-Centric Age<br>nts in Embodied Instruction Following | [ArXiv](https://arxiv.org/pdf/2403.03017) |  |
| Agent, Embodied | OpenAgents: An Open Platform for Language Agents i<br>n the Wild | [ArXiv](https://arxiv.org/abs/2310.10634), [GitHub](https://github.com/xlang-ai/OpenAgents) |  |
| Agent, Embodied | LLM-Planner: Few-Shot Grounded Planning for Embodi<br>ed Agents with Large Language Models | [ArXiv](https://arxiv.org/pdf/2212.04088) |  |
| Agent, Embodied | Embodied Multi-Modal Agent trained by an LLM from <br>a Parallel TextWorld | [ArXiv](https://arxiv.org/abs/2311.16714) |  |
| Agent, Embodied | Octopus: Embodied Vision-Language Programmer from <br>Environmental Feedback |  |  |
| Agent, Embodied | Embodied Task Planning with Large Language Models | [ArXiv](https://arxiv.org/pdf/2307.01848) |  |
| Agent, Diffusion, Speech | NaturalSpeech 3: Zero-Shot Speech Synthesis with F<br>actorized Codec and Diffusion Models | [ArXiv](https://arxiv.org/pdf/2403.03100) |  |
| Agent, Code-as-Policies | Executable Code Actions Elicit Better LLM Agents | [ArXiv](https://arxiv.org/abs/2402.01030) | 2024/01/24 |
| Agent, Code-LLM, Code-as-Policies, Survey | If LLM Is the Wizard, Then Code Is the Wand: A Sur<br>vey on How Code Empowers Large Language Models to Serve as Intelligent Agents | [ArXiv](https://arxiv.org/abs/2401.00812) |  |
| Agent, Code-LLM | TaskWeaver: A Code-First Agent Framework |  |  |
| Agent, Blog | LLM Powered Autonomous Agents | [ArXiv](https://lilianweng.github.io/posts/2023-06-23-agent/) |  |
| Agent, Awesome Repo, LLM | Awesome-Embodied-Agent-with-LLMs | [GitHub](https://github.com/zchoi/Awesome-Embodied-Agent-with-LLMs) |  |
| Agent, Awesome Repo, LLM | CoALA: Awesome Language Agents | [ArXiv](https://arxiv.org/pdf/2309.02427), [GitHub](https://github.com/ysymyth/awesome-language-agents) |  |
| Agent, Awesome Repo, Embodied, Grounding | XLang Paper Reading | [GitHub](https://github.com/xlang-ai/xlang-paper-reading) |  |
| Agent, Awesome Repo | Awesome AI Agents | [GitHub](https://github.com/e2b-dev/awesome-ai-agents) |  |
| Agent, Awesome Repo | Autonomous Agents | [GitHub](https://github.com/tmgthb/Autonomous-Agents) |  |
| Agent, Awesome Repo | Awesome-Papers-Autonomous-Agent | [GitHub](https://gh.mlsub.net/lafmdp/Awesome-Papers-Autonomous-Agent) |  |
| Agent, Awesome Repo | Awesome Large Multimodal Agents | [GitHub](https://github.com/jun0wanan/awesome-large-multimodal-agents) |  |
| Agent, Awesome Repo | LLM Agents Papers | [GitHub](https://github.com/zjunlp/LLMAgentPapers) |  |
| Agent, Awesome Repo | Awesome LLM-Powered Agent | [GitHub](https://github.com/hyp1231/awesome-llm-powered-agent) |  |
| Agent | XAgent: An Autonomous Agent for Complex Task Solvi<br>ng |  |  |
| Agent | LLM-Powered Hierarchical Language Agent for Real-t<br>ime Human-AI Coordination | [ArXiv](https://arxiv.org/pdf/2312.15224) |  |
| Agent | AgentVerse: Facilitating Multi-Agent Collaboration<br> and Exploring Emergent Behaviors | [ArXiv](https://arxiv.org/abs/2308.10848) |  |
| Agent | Agents: An Open-source Framework for Autonomous La<br>nguage Agents | [ArXiv](https://arxiv.org/abs/2309.07870), [GitHub](https://github.com/aiwaves-cn/agents) |  |
| Agent | AutoAgents: A Framework for Automatic Agent Genera<br>tion | [GitHub](https://github.com/Link-AGI/AutoAgents) |  |
| Agent | DSPy: Compiling Declarative Language Model Calls i<br>nto Self-Improving Pipelines | [ArXiv](https://arxiv.org/abs/2310.03714) |  |
| Agent | AutoGen: Enabling Next-Gen LLM Applications via Mu<br>lti-Agent Conversation | [ArXiv](https://arxiv.org/pdf/2308.08155) |  |
| Agent | CAMEL: Communicative Agents for “Mind” Exploration<br> of Large Language Model Society | [ArXiv](https://arxiv.org/pdf/2303.17760) |  |
| Agent | XAgent: An Autonomous Agent for Complex Task Solvi<br>ng | [ArXiv](https://blog.x-agent.net/blog/xagent/) |  |
| Agent | Generative Agents: Interactive Simulacra of Human <br>Behavior | [ArXiv](https://arxiv.org/abs/2304.03442) |  |
| Agent | LLM+P: Empowering Large Language Models with Optim<br>al Planning Proficiency | [ArXiv](https://arxiv.org/abs/2304.11477) | 2023/04/22 |
| Agent | AgentSims: An Open-Source Sandbox for Large Langua<br>ge Model Evaluation | [ArXiv](https://arxiv.org/abs/2308.04026) | 2023/08/08 |
| Agent | Agents: An Open-source Framework for Autonomous La<br>nguage Agents | [ArXiv](https://arxiv.org/pdf/2309.07870) |  |
| Agent | MindAgent: Emergent Gaming Interaction | [ArXiv](https://arxiv.org/pdf/2309.09971) |  |
| Agent | InfiAgent: A Multi-Tool Agent for AI Operating Sys<br>tems |  |  |
| Agent | Predictive Minds: LLMs As Atypical Active Inferenc<br>e Agents |  |  |
| Agent | swarms | [GitHub](https://github.com/kyegomez/swarms) |  |
| Agent | ScreenAgent: A Vision Language Model-driven Comput<br>er Control Agent | [ArXiv](https://arxiv.org/pdf/2402.07945) |  |
| Agent | AssistGPT: A General Multi-modal Assistant that ca<br>n Plan, Execute, Inspect, and Learn | [ArXiv](https://arxiv.org/pdf/2306.08640) |  |
| Agent | PromptAgent: Strategic Planning with Language Mode<br>ls Enables Expert-level Prompt Optimization | [ArXiv](https://arxiv.org/pdf/2310.16427) |  |
| Agent | Cognitive Architectures for Language Agents | [ArXiv](https://arxiv.org/abs/2309.02427) |  |
| Agent | AIOS: LLM Agent Operating System | [ArXiv](https://arxiv.org/abs/2403.16971) |  |
| Agent | LLM as OS, Agents as Apps: Envisioning AIOS, Agent<br>s and the AIOS-Agent Ecosystem | [ArXiv](https://arxiv.org/abs/2312.03815) |  |
| Agent | Towards General Computer Control: A Multimodal Age<br>nt for Red Dead Redemption II as a Case Study |  |  |
| Affordance, Segmentation | ManipVQA: Injecting Robotic Affordance and Physica<br>lly Grounded Information into Multi-Modal Large Language Models | [ArXiv](https://arxiv.org/abs/2403.11289) |  |
| Action-Model, Agent, LAM | LaVague | [GitHub](https://github.com/lavague-ai/LaVague) |  |
| Action-Generation, Generation, Prompting | Prompt a Robot to Walk with Large Language Models | [ArXiv](https://arxiv.org/pdf/2309.09969) |  |
| APIs, Agent, Tool | Gorilla: Large Language Model Connected with Massi<br>ve APIs | [ArXiv](https://arxiv.org/abs/2305.15334) |  |
| AGI, Survey | Levels of AGI: Operationalizing Progress on the Pa<br>th to AGI | [ArXiv](https://arxiv.org/pdf/2311.02462) |  |
| AGI, Brain | When Brain-inspired AI Meets AGI | [ArXiv](https://arxiv.org/pdf/2305.19352) |  |
| AGI, Brain | Divergences between Language Models and Human Brai<br>ns | [ArXiv](https://arxiv.org/pdf/2311.09308) |  |
| AGI, Awesome Repo, Survey | Awesome-LLM-Papers-Toward-AGI | [GitHub](https://github.com/shure-dev/Awesome-LLM-Papers-Toward-AGI) |  |
| AGI, Agent | OpenAGI: When LLM Meets Domain Experts |  |  |
| 3D, Open-source, Perception, Robot | 3D-LLM: Injecting the 3D World into Large Language<br> Models | [ArXiv](https://arxiv.org/abs/2307.12981) | 2023/07/24 |
| 3D, GPT4, VLM | GPT-4V(ision) is a Human-Aligned Evaluator for Tex<br>t-to-3D Generation | [ArXiv](https://arxiv.org/abs/2401.04092) |  |
|  | ChatEval: Towards Better LLM-based Evaluators thro<br>ugh Multi-Agent Debate | [ArXiv](https://arxiv.org/abs/2308.07201) | 2023/08/14 |
